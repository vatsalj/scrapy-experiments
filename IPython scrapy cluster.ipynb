{
 "metadata": {
  "name": "",
  "signature": "sha256:47c324e7a49f5c24ffb825391acfcdadd089dc4917af603dd69d7e7438fb7396"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Distributed hello world\n",
      "\n",
      "Originally by Ken Kinder (ken at kenkinder dom com)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from __future__ import print_function\n",
      "import os\n",
      "import sys\n",
      "import multiprocessing\n",
      "from multiprocessing.queues import Queue\n",
      "import lxml.etree\n",
      "import lxml.html\n",
      "import urlparse\n",
      "\n",
      "from IPython.parallel import Client\n",
      "\n",
      "from scrapy import project, signals\n",
      "from scrapy.spider import Spider\n",
      "from scrapy.item import Item, Field\n",
      "from scrapy.crawler import CrawlerProcess\n",
      "from scrapy.xlib.pydispatch import dispatcher\n",
      "from scrapy.utils.project import get_project_settings\n",
      "from scrapy.http import Request\n",
      "from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n",
      "\n",
      "TMP_DIR = './tmp'\n",
      "\n",
      "class ResponseItem(Item):\n",
      "    response = Field()\n",
      "\n",
      "class ResponseSpider(Spider):\n",
      "    name = 'response_spider'\n",
      "    \n",
      "    def __init__(self, url):\n",
      "        self.client = Client()\n",
      "        self.view = self.client.load_balanced_view()\n",
      "        self.allUrls = [url]\n",
      "        # :TODO: Add self.workingUrls\n",
      "        self.workingUrls = {}\n",
      "        self.doneUrls = []\n",
      "        super(ResponseSpider, self).__init__()\n",
      "        \n",
      "    def start_requests(self):\n",
      "        new_url = self.newUrls.pop()\n",
      "        request = [Request(new_url, self.parse, dont_filter=True)]\n",
      "        return request\n",
      "        \n",
      "    def parse(self, response):\n",
      "        # request with callback of own instance fails to serialize\n",
      "        # code can be made functional using copy_reg\n",
      "        # see: http://bytes.com/topic/python/answers/552476-why-cant-you-pickle-instancemethods\n",
      "        self.doneUrls.append(response.url)\n",
      "        hxs = HtmlXPathSelector(response)\n",
      "        urls = hxs.select('//a').extract()\n",
      "        self.workingUrls[response.url] = []\n",
      "        for url in urls:\n",
      "            next_link = urlparse.urljoin(response.url, url)\n",
      "            self.workingUrls[response.url].append(next_link)\n",
      "            yield Request(url, callback=self.parseNextLink)\n",
      "                \n",
      "    def parseNextLink(self, response):\n",
      "        return ResponseItem(response)\n",
      "\n",
      "\n",
      "class CrawlerWorker(multiprocessing.Process):\n",
      "    def __init__(self, result_queue, spider, settings=None):\n",
      "        multiprocessing.Process.__init__(self)\n",
      "        self.settings = settings or get_project_settings()\n",
      "        self.result_queue = result_queue\n",
      "        self.spider = spider\n",
      "        self.items = []\n",
      "        dispatcher.connect(self._item_scraped, signals.item_scraped)\n",
      "         \n",
      "    def _item_scraped(self, item):\n",
      "        self.items.append(item)\n",
      "  \n",
      "    def run(self):\n",
      "        self.crawler_process = CrawlerProcess(self.settings)\n",
      "        self.crawler = self.crawler_process.create_crawler()\n",
      "        self.crawler.crawl(self.spider)\n",
      "        self.crawler_process.start()\n",
      "        self.crawler_process.stop()\n",
      "        self.result_queue.put(self.items)\n",
      "        \n",
      "\n",
      "def _download(url):\n",
      "    result_queue = Queue()\n",
      "    spider = ResponseSpider(url)\n",
      "    crawler = CrawlerWorker(result_queue, spider)\n",
      "    crawler.start()            \n",
      "    item = result_queue.get()[0]\n",
      "    result_queue.cancel_join_thread()\n",
      "    crawler.join()\n",
      "    return item['response']\n",
      "\n",
      "def set_base(body, base):\n",
      "    if '<base' not in body:\n",
      "        body = body.replace('<head>', '<head><base href=\"%s\">' % base)\n",
      "    return body\n",
      "\n",
      "def download(url):\n",
      "    \"\"\"\n",
      "    Download 'url' using Scrapy. Return Response.\n",
      "    \"\"\"\n",
      "    response = _download(url)\n",
      "    return response.replace(body=set_base(response.body, url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "rc = Client()\n",
      "view = rc.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def sleep_and_echo(t, msg):\n",
      "    import time\n",
      "    time.sleep(t)\n",
      "    return msg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "world = view.apply_async(sleep_and_echo, 3, 'World!')\n",
      "hello = view.apply_async(sleep_and_echo, 2, 'Hello')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Submitted tasks:\", hello.msg_ids + world.msg_ids)\n",
      "print(hello.get(), world.get())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Submitted tasks: ['eb7dfbbf-7bad-45fe-915d-5d538233de42', '15d2fe60-c55a-458a-9cdc-55a64185a64f']\n",
        "Hello"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " World!\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rc.ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[0, 1, 2, 3]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}