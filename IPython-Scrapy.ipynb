{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import os\n",
      "import sys\n",
      "import multiprocessing\n",
      "from multiprocessing.queues import Queue\n",
      "import lxml.etree\n",
      "import lxml.html\n",
      "from scrapy import project, signals\n",
      "from scrapy.spider import Spider\n",
      "from scrapy.item import Item, Field\n",
      "from scrapy.crawler import CrawlerProcess\n",
      "from scrapy.xlib.pydispatch import dispatcher\n",
      "from scrapy.utils.project import get_project_settings\n",
      "from scrapy.http import Request\n",
      "from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector\n",
      "\n",
      "TMP_DIR = './tmp'\n",
      "\n",
      "class ResponseItem(Item):\n",
      "    response = Field()\n",
      "\n",
      "class ResponseSpider(Spider):\n",
      "    name = 'response_spider'\n",
      "    \n",
      "    def __init__(self, url):\n",
      "        self.url = url\n",
      "        super(ResponseSpider, self).__init__()\n",
      "        \n",
      "    def start_requests(self):\n",
      "        return [Request(self.url, self.parse, dont_filter=True)]\n",
      "        \n",
      "    def parse(self, response):\n",
      "        # request with callback fails to serialize - why?\n",
      "        req = response.request.replace(callback=None)\n",
      "        return ResponseItem(\n",
      "            response=response.replace(request=req),\n",
      "        )\n",
      " \n",
      "    \n",
      "class CrawlerWorker(multiprocessing.Process):\n",
      "    def __init__(self, result_queue, spider, settings=None):\n",
      "        multiprocessing.Process.__init__(self)\n",
      "        self.settings = settings or get_project_settings()\n",
      "        self.result_queue = result_queue\n",
      "        self.spider = spider\n",
      "        self.items = []\n",
      "        dispatcher.connect(self._item_passed, signals.item_scraped)\n",
      "         \n",
      "    def _item_passed(self, item):\n",
      "        self.items.append(item)\n",
      "  \n",
      "    def run(self):\n",
      "        self.crawler_process = CrawlerProcess(self.settings)\n",
      "        self.crawler = self.crawler_process.create_crawler()\n",
      "        self.crawler.crawl(self.spider)\n",
      "        self.crawler_process.start()\n",
      "        self.crawler_process.stop()\n",
      "        self.result_queue.put(self.items)\n",
      "        \n",
      "\n",
      "def _download(url):\n",
      "    result_queue = Queue()\n",
      "    spider = ResponseSpider(url)\n",
      "    crawler = CrawlerWorker(result_queue, spider)\n",
      "    crawler.start()            \n",
      "    item = result_queue.get()[0]\n",
      "    result_queue.cancel_join_thread()\n",
      "    crawler.join()\n",
      "    return item['response']\n",
      "\n",
      "def set_base(body, base):\n",
      "    if '<base' not in body:\n",
      "        body = body.replace('<head>', '<head><base href=\"%s\">' % base)\n",
      "    return body\n",
      "\n",
      "def download(url):\n",
      "    \"\"\"\n",
      "    Download 'url' using Scrapy. Return Response.\n",
      "    \"\"\"\n",
      "    response = _download(url)\n",
      "    return response.replace(body=set_base(response.body, url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython import display\n",
      "\n",
      "def _show_in_iframe(local_url):\n",
      "    fname = os.path.join(TMP_DIR, 'output.html')\n",
      "    html = \"\"\"<html><body>\n",
      "    <p><input type='button' value='Do we need'> <input type='button' value='some UI controls?'></p>\n",
      "    <hr>\n",
      "    <iframe style='width:800px; height:600px;' src=\"%s\"></iframe>\n",
      "    </body></html>\"\"\" % local_url\n",
      "    display.display(display.HTML(html))\n",
      "\n",
      "\n",
      "def show_in_iframe(html):\n",
      "    fname = os.path.join(TMP_DIR, 'output.html')\n",
      "    with open(fname, 'wb') as f:        \n",
      "        f.write(html)            \n",
      "    _show_in_iframe('http://127.0.0.1:8000/output.html')\n",
      "        \n",
      "\n",
      "def _highlight(hxs):\n",
      "    el = hxs._root\n",
      "    el.attrib['style'] = 'background-color: yellow;' + el.get('style', '')    \n",
      "\n",
      "\n",
      "def show_hxs_select(hxs, xpath):\n",
      "    for link in hxs.select(xpath):\n",
      "        _highlight(link)\n",
      "    \n",
      "    body = lxml.html.tostring(hxs._root.getroottree())\n",
      "    show_in_iframe(body)\n",
      "\n",
      "    \n",
      "def show_xpath(url, xpath):\n",
      "    response = download(url)\n",
      "    hxs = HtmlXPathSelector(response)\n",
      "    show_hxs_select(hxs, xpath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_xpath('http://crawlera.com', '//a[contains(text(), \"i\")]')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-03-23 12:44:06+0530 [scrapy] INFO: Scrapy 0.23.0 started (bot: tutorial)\n",
        "2014-03-23 12:44:06+0530 [scrapy] INFO: Optional features available: ssl, http11\n",
        "2014-03-23 12:44:06+0530 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial'}\n",
        "2014-03-23 12:44:06+0530 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\n",
        "2014-03-23 12:44:06+0530 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
        "2014-03-23 12:44:06+0530 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
        "2014-03-23 12:44:06+0530 [scrapy] INFO: Enabled item pipelines: \n",
        "2014-03-23 12:44:06+0530 [response_spider] INFO: Spider opened\n",
        "2014-03-23 12:44:06+0530 [response_spider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
        "2014-03-23 12:44:06+0530 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023\n",
        "2014-03-23 12:44:06+0530 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080\n",
        "2014-03-23 12:44:07+0530 [response_spider] DEBUG: Crawled (200) <GET http://crawlera.com> (referer: None)\n",
        "2014-03-23 12:44:07+0530 [response_spider] DEBUG: Scraped from <200 http://crawlera.com>\n",
        "\t{'response': <200 http://crawlera.com>}\n",
        "2014-03-23 12:44:07+0530 [response_spider] INFO: Closing spider (finished)\n",
        "2014-03-23 12:44:07+0530 [response_spider] INFO: Dumping Scrapy stats:\n",
        "\t{'downloader/request_bytes': 211,\n",
        "\t 'downloader/request_count': 1,\n",
        "\t 'downloader/request_method_count/GET': 1,\n",
        "\t 'downloader/response_bytes': 2867,\n",
        "\t 'downloader/response_count': 1,\n",
        "\t 'downloader/response_status_count/200': 1,\n",
        "\t 'finish_reason': 'finished',\n",
        "\t 'finish_time': datetime.datetime(2014, 3, 23, 7, 14, 7, 783423),\n",
        "\t 'item_scraped_count': 1,\n",
        "\t 'log_count/DEBUG': 4,\n",
        "\t 'log_count/INFO': 7,\n",
        "\t 'response_received_count': 1,\n",
        "\t 'scheduler/dequeued': 1,\n",
        "\t 'scheduler/dequeued/memory': 1,\n",
        "\t 'scheduler/enqueued': 1,\n",
        "\t 'scheduler/enqueued/memory': 1,\n",
        "\t 'start_time': datetime.datetime(2014, 3, 23, 7, 14, 6, 573833)}\n",
        "2014-03-23 12:44:07+0530 [response_spider] INFO: Spider closed (finished)\n"
       ]
      },
      {
       "html": [
        "<html><body>\n",
        "    <p><input type='button' value='Do we need'> <input type='button' value='some UI controls?'></p>\n",
        "    <hr>\n",
        "    <iframe style='width:800px; height:600px;' src=\"http://127.0.0.1:8000/output.html\"></iframe>\n",
        "    </body></html>"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x2f4d490>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}